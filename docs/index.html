<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning">
  <meta name="keywords" content="COMPACT, AI, Multimodal, Vision, Language, Compositionality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning</title>

  <script src="https://kit.fontawesome.com/854f31a6c4.js" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <style>
    .header-container {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 10px;
    }
    .header-container img {
      height: 50px;
    }
    .header-container h1 {
      margin: 0;
    }
    .conference-text {
      color: #4A90E2;
      font-weight: bold;
    }
    .hero-body {
      padding: 3rem 1.5rem;
    }
    .small-centered-image {
      display: block;
      margin-left: auto;
      margin-right: auto;
      max-width: 800px;
      width: 100%;
    }
    .table-container {
      display: flex;
      justify-content: center;
    }
    .publication-title {
      font-size: 2.5rem;
    }
    .publication-authors {
      margin-bottom: 1rem;
    }
    .publication-links {
      margin-top: 1.5rem;
    }
    .content img {
      margin: 1.5rem auto;
      display: block;
    }
    .section {
      padding: 3rem 1.5rem;
    }
    .fancy_text_color {
      color: #4A90E2;
    }
    /* Custom styling for pipeline image */
    .pipeline-image {
      max-width: 800px;
      width: 100%;
      margin: 0 auto 2rem auto;
    }
    footer {
      padding: 3rem 1.5rem 3rem;
    }
    /* New styles for side-by-side layout */
    .flex-container {
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 20px;
    }
    .flex-container img {
      max-width: 300px; /* Adjust the size as needed */
      width: 100%;
    }
  </style>
</head>
  
<body>
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://princetonvisualai.github.io/compact/">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">More Research</a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://princetonvisualai.github.io/conceptmix/">ConceptMix</a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">COMPACT</h1>
          <h2 class="subtitle is-3 publication-subtitle">COMPositional Atomic-to-Complex Visual Capability Tuning</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://xindiwu.github.io/">Xindi Wu</a><sup>*</sup>,</span>
            <span class="author-block"><a href="#">Hee Seung Hwang</a><sup>*</sup>,</span>
            <span class="author-block"><a href="https://polkirichenko.github.io/">Polina Kirichenko</a>,</span>
            <span class="author-block"><a href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Princeton University, Meta AI</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block" style="font-size: 0.9em; color: #666;">* Equal Contribution</span>
          </div>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block conference-text">[NeurIPS 2024 D&B]</span> -->
          </div>
          <div class="publication-links">
            <span class="link-block">
              <a href="#" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://github.com/princetonvisualai/compact" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
              </a>
            </span>
            <span class="link-block">
              <a href="#" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-powerpoint"></i></span>
                <span>Slides</span>
              </a>
            </span>
            <span class="link-block">
              <a href="#" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-image"></i></span>
                <span>Poster</span>
              </a>
            </span>
            <span class="link-block">
              <a href="#" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-external-link-alt"></i></span>
                <span>OpenReview</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="./static/images/1.png" alt="COMPACT Pipeline Figure" class="pipeline-image">
      <h2 class="subtitle has-text-centered">
        COMPACT provides a flexible and scalable data recipe for visual compositional tuning.
      </h2>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <!-- Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as object recognition, spatial understanding, and counting. Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but has overlooked the compositional complexity of training examples, limiting their effectiveness in real-world scenarios.  -->
          </p>
          <p>
            We propose COMPACT (COMPositional Atomic-to-complex Visual Capability Tuning), 
            a data recipe that explicitly controls for the compositional complexity of the training examples. 
            COMPACT data allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. 
            Across complex multi-capability benchmarks, COMPACT outperforms the LLaVA-665K VIT 
            while using less than 10% of its data budget. 
          </p>
          <p>
            Training on COMPACT data, whose samples require up to three atomic capabilities, 
            generalizes well to tasks that have even higher capability requirements. 
            For example, COMPACT achieves a substantial 83.3% improvement on MMStar and 94.0% improvement 
            on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. 
            COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/2.png" alt="Comparison of current VIT and COMPACT" class="small-centered-image">
          <p>
            <b>Current Visual Instruction Tuning Data vs. Compositional Tuning Data:</b> 
            The VIT data is dominated by simple queries (k = 1), while our COMPACT data is balanced 
            across compositional complexity levels (k = 1, 2, 3).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            We propose COMPACT, a data recipe that scales capabilities of MLLMs from atomic (k = 1) to composite (k > 1) complexity levels. 
            Our approach generates multi-capability training data by prompting vision-language models to create questions that 
            integrate k = (1, 2, 3) atomic visual capabilities.
          </p>
          
          <div class="has-text-centered">
            <img src="./static/images/1.png" alt="COMPACT Data Generation Pipeline" class="small-centered-image">
          </div>
          <p>
            <b>COMPACT Data Generation Pipeline:</b> (Left): We sample atomic capabilities (k = 1) such as color, object recognition, and spatial relationship. 
            (Center): We generate questions (k = 1, 2, 3) that integrate all the sampled capabilities. 
            (Right): We verify the quality of generated conversations and combine them with instruction tuning data to maintain instruction following capability.
          </p>
        </div>
        
        <h3 class="title is-4">Atomic Visual Capabilities</h3>
        <div class="content has-text-justified flex-container">
          <p>
            Atomic capabilities are foundational skills that can be combined to solve complex tasks. 
            For example, a model needs to acquire object recognition, color attribution, and spatial relationship understanding capabilities 
            to identify how objects of different colors are spatially oriented. We define the number of atomic capabilities required to solve 
            a task as its compositional complexity k.
          </p>
          <img src="./static/images/3.png" alt="Atomic Visual Capabilities" class="small-centered-image">
        </div>
        
        <p>
          We identify 10 atomic capabilities and categorize them into three groups:
        </p>
        
        <table class="table is-bordered is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Group</th>
              <th>Capability</th>
              <th>Definition</th>
              <th>Example Question</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="2">Attribution</td>
              <td>Color</td>
              <td>Identifying or comparing colors of objects in the image</td>
              <td>What color is the car?</td>
            </tr>
            <tr>
              <td>Shape</td>
              <td>Recognizing and describing the shapes of objects in the image</td>
              <td>What shape is the dining table?</td>
            </tr>
            <tr>
              <td rowspan="5">Recognition</td>
              <td>Object Recognition</td>
              <td>Identifying and naming objects present in the image</td>
              <td>What object is on the table?</td>
            </tr>
            <tr>
              <td>Action Recognition</td>
              <td>Identifying what action is being performed</td>
              <td>What is the person doing in this image?</td>
            </tr>
            <tr>
              <td>Text Recognition</td>
              <td>Reading and interpreting text visible in the image</td>
              <td>What word is written on the sign?</td>
            </tr>
            <tr>
              <td>Spatial Recognition</td>
              <td>Understanding the overall spatial layout and arrangement of the entire scene</td>
              <td>How is the furniture arranged in this room?</td>
            </tr>
            <tr>
              <td>Counting</td>
              <td>Determining the number of instances of something in the image</td>
              <td>How many people are in the room?</td>
            </tr>
            <tr>
              <td rowspan="3">Relation</td>
              <td>Spatial Relationship</td>
              <td>Identifying how specific objects are positioned relative to each other</td>
              <td>What is next to the red car?</td>
            </tr>
            <tr>
              <td>Object Interaction</td>
              <td>Analyzing how multiple objects interact with each other</td>
              <td>How is the woman interacting with the laptop?</td>
            </tr>
            <tr>
              <td>Scene Understanding</td>
              <td>Identifying the type of environment/setting</td>
              <td>Where is this scene taking place?</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        
        <div class="content has-text-justified">
          <p>
            With 32K samples of our compositional tuning data and 5% of the LLaVA-665K VIT data (only 10% of the size of the full VIT dataset), 
            COMPACT matches the performance of full-scale VIT (100.18% relative score) and demonstrates exceptional generalization to complex tasks.
          </p>
          
          <div class="has-text-centered">
            <p>
              <b>Baseline Comparisons:</b> Performance comparison of COMPACT with baselines. COMPACT (65K) outperforms the random subset of 
              the VIT data (65K), gradient-based approach selected subset of the VIT data (65K), and even the full VIT data (665K) on 
              diverse multimodal benchmarks. The best and second best results for each benchmark are shown in <strong>bold</strong> and
               <u>underlined</u>, respectively. COMPACT integrates atomic capabilities into tasks of higher compositional complexity, 
               enabling models to generalize and handle complex tasks without explicit decomposition.
            </p>
            <div style="overflow-x: auto;">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Recipe</th>
                    <th>#Data</th>
                    <th>InfoVQA</th>
                    <th>SeedBench2Plus</th>
                    <th>MME</th>
                    <th>TextVQA</th>
                    <th>MM-Vet</th>
                    <th>CV-Bench</th>
                    <th>MMStar</th>
                    <th>LLaVA-W</th>
                    <th>Rel. (%)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Original</td>
                    <td>665K</td>
                    <td>20.80</td>
                    <td>41.72</td>
                    <td><strong>1478.48</strong></td>
                    <td><strong>46.99</strong></td>
                    <td>29.22</td>
                    <td><strong>60.92</strong></td>
                    <td>35.11</td>
                    <td><strong>68.50</strong></td>
                    <td>100.00</td>
                  </tr>
                  <tr>
                    <td>Random</td>
                    <td>65K</td>
                    <td>20.05</td>
                    <td>41.85</td>
                    <td>1327.70</td>
                    <td>42.88</td>
                    <td>30.46</td>
                    <td>54.71</td>
                    <td>34.13</td>
                    <td>64.30</td>
                    <td>95.38</td>
                  </tr>
                  <tr>
                    <td>ICONS</td>
                    <td>65K</td>
                    <td><u>21.0</u></td>
                    <td><u>42.03</u></td>
                    <td><u>1402.75</u></td>
                    <td>43.12</td>
                    <td><u>31.23</u></td>
                    <td><u>55.96</u></td>
                    <td><u>35.96</u></td>
                    <td>61.8</td>
                    <td>97.47</td>
                  </tr>
                  <tr>
                    <td>COMPACT (ours)</td>
                    <td>65K</td>
                    <td><strong>23.68</strong></td>
                    <td><strong>43.13</strong></td>
                    <td>1379.94</td>
                    <td><u>44.37</u></td>
                    <td><strong>31.74</strong></td>
                    <td>55.28</td>
                    <td><strong>36.13</strong></td>
                    <td><u>64.50</u></td>
                    <td>100.18</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          
          <!-- <div class="has-text-centered">
            <img src="/api/placeholder/800/400" alt="Performance Across Complexity" class="small-centered-image">
          </div> -->
          <p>
            <b>Compositional Generalization to Higher Complexities:</b> We compare the performance of COMPACT (65K) and LLaVA-665K VIT (665K) at each 
            compositional complexity (k) level. COMPACT exceeds the LLaVA-665K baseline at k = (3,4,5) tasks while using significantly less training data.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/compositional_generalization.png" alt="Performance Across Compositional Tuning Data Scales" class="small-centered-image">
          </div>
          <p>
            <b>Performance Across Compositional Tuning Data Scales:</b> We fix the VIT subset (5% of LLaVA-665K) and scale the
            compositional tuning data in COMPACT from 2K to 32K. For comparison, we remove the compositional tuning data and add more VIT data
            (2K-32K) instead to prepare VIT only baselines with equal data budgets. With much fewer data, COMPACT (solid lines) consistently outperforms the 
            VIT only baselines (dashed lines). The performance gap is pronounced for complex reasoning benchmarks such as MM-Vet and MMStar,
            where the 8K COMPACT model often exceeds the VIT only baseline at 32K. This demonstrates the data efficiency of
            COMPACT, requiring substantially less data than LLaVA-665K VIT to achieve comparable or better results.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/compositional_data_scale.png" alt="Performance Across Compositional Tuning Data Scales" class="small-centered-image">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablations</h2>
        
        <div class="content has-text-justified">
          <p>
            We conduct extensive ablation studies to understand the impact of different components in COMPACT:
          </p>
          
          <p>
            <b>Matching Llava Dist:</b> We analyze how varying the maximum compositional complexity (k) in our training data affects model performance. Training with k=3 achieves the best balance between performance and data efficiency, while higher k values show diminishing returns.
          </p>
          <p>
            Put a table here
          </p>
          <p>
            <b>Atomic Capability Coverage:</b> We examine how the distribution of atomic capabilities in our training data influences model performance. A balanced distribution across all capability types proves most effective, highlighting the importance of comprehensive capability coverage.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/loo.png" alt="Atomic Capability Coverage" class="small-centered-image">
          </div>
          <p>
            <b>compositional complexity range:</b> Our quality verification step significantly improves performance by ensuring generated questions accurately reflect intended capability combinations and removing low-quality samples.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/k_range_scale.png" alt="Compositional Complexity Range" class="small-centered-image">
          </div>
          <p>
            <b>VIT Ratio:</b> Our quality verification step significantly improves performance by ensuring generated questions accurately reflect intended capability combinations and removing low-quality samples.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/vit_scale.png" alt="VIT Data Ratio" class="small-centered-image">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Comparison</h2>
        
        <div class="content has-text-justified">
          <p>
            We perform detailed quantitative analysis comparing COMPACT with baseline models across different capability requirements and benchmarks:
          </p>

          <div class="has-text-centered">
            <img src="./static/images/heatmap.png" alt="Capability Distribution" class="small-centered-image">
          </div>
          <div class="has-text-centered">
            <img src="./static/images/llava_k_dist.png" alt="Llava k-distribution" class="small-centered-image">
          </div>


        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Comparison</h2>
        
        <div class="content has-text-justified">
          <p>
            Below we showcase examples that demonstrate the superior performance of our COMPACT model compared to LLaVA-665K VIT,
            particularly on complex queries requiring multiple capabilities (k ≥ 3).
          </p>

          <div class="has-text-centered">
            <img src="./static/images/qualitative_1.png" alt="Qualitative Examples 1" class="small-centered-image">
          </div>
          <div class="has-text-centered">
            <img src="./static/images/qualitative_2.png" alt="Qualitative Examples 2" class="small-centered-image">
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wu2024compact,
  title={COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning},
  author={Wu, Xindi and Hwang, Hee Seung and Kirichenko, Polina and Russakovsky, Olga},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      <strong>COMPACT</strong> by <a href="https://visualai.princeton.edu/">Princeton University Visual AI Lab</a>
    </p>
  </div>
</footer>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
    if ($navbarBurgers.length > 0) {
      $navbarBurgers.forEach(el => {
        el.addEventListener('click', () => {
          const target = el.dataset.target;
          const $target = document.getElementById(target);
          el.classList.toggle('is-active');
          $target.classList.toggle('is-active');
        });
      });
    }
  });
</script>
</body>
</html>